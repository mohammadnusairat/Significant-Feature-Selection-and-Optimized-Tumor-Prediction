# Compute confusion matrix
conf_matrix <- confusionMatrix(knn_predictions, knn_test_data$Diagnosis)
# Extract confusion matrix as a table
conf_table <- as.data.frame(conf_matrix$table)
# Rename columns for clarity
colnames(conf_table) <- c("Prediction", "Reference", "Freq")
# Create a heatmap of the confusion matrix
ggplot(conf_table, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
ggtitle("Confusion Matrix Heatmap for KNN") +
xlab("Actual Class") +
ylab("Predicted Class") +
theme_minimal()
# Step 1: Identify the best kernel
kernels <- c("linear", "radial", "polynomial", "sigmoid")
results <- lapply(kernels, function(kernel) {
evaluate_svm_kernel(kernel, train_data, test_data, "Diagnosis")
})
evaluate_svm_kernel <- function(kernel, train_data, test_data, target_column) {
set.seed(123)
model <- svm(
as.formula(paste(target_column, "~ .")),
data = train_data,
kernel = kernel,
probability = TRUE
)
predictions <- predict(model, test_data)
accuracy <- mean(predictions == test_data[[target_column]])
list(kernel = kernel, accuracy = accuracy)
}
# Step 1: Identify the best kernel
kernels <- c("linear", "radial", "polynomial", "sigmoid")
results <- lapply(kernels, function(kernel) {
evaluate_svm_kernel(kernel, train_data, test_data, "Diagnosis")
})
accuracy_results <- data.frame(
Kernel = sapply(results, function(res) res$kernel),
Accuracy = sapply(results, function(res) res$accuracy)
)
best_kernel <- accuracy_results[which.max(accuracy_results$Accuracy), "Kernel"]
cat("Best Kernel:", best_kernel, "\n")
# Step 2: Hyperparameter tune the best kernel
set.seed(123)
tuned_model <- tune(
svm,
Diagnosis ~ .,
data = train_data,
kernel = best_kernel,
ranges = list(
cost = 2^(-1:2),
gamma = 2^(-2:1)
)
)
best_model <- tuned_model$best.model
best_parameters <- tuned_model$best.parameters
cat("Best Parameters for Kernel", best_kernel, ":\n")
print(best_parameters)
# Step 3: Evaluate the tuned model
tuned_predictions <- predict(best_model, test_data)
confusion_matrix <- table(Predicted = tuned_predictions, Actual = test_data$Diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Tuned Model Accuracy:", accuracy, "\n")
# Convert confusion matrix to a dataframe for visualization
confusion_df <- as.data.frame(as.table(confusion_matrix))
colnames(confusion_df) <- c("Predicted", "Actual", "Freq")
# Create the heatmap
ggplot(confusion_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
ggtitle("Confusion Matrix Heatmap for Tuned SVM Model") +
xlab("Actual Class") +
ylab("Predicted Class") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(reshape2)
library(rpart)
library(rpart.plot)
library(e1071)
library(car)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(class)
library(reshape2)
setwd("\\Users\\mnusa\\STAT 385\\Final Project")
wdbc.data <- read.table(file="wdbc.data",sep = ",",header=FALSE)
str(wdbc.data)
ncol(wdbc.data)
sum(is.na(wdbc.data))
#renaming the columns, to make sense
column_names <- c(
"ID", "Diagnosis",
paste(rep(c("Mean", "SE", "Worst"), each = 10),
c("Radius", "Texture", "Perimeter", "Area",
"Smoothness", "Compactness", "Concavity",
"ConcavePoints", "Symmetry", "FractalDimension"), sep = "_")
)
colnames(wdbc.data) <- column_names
wdbc.data$Diagnosis <- factor(wdbc.data$Diagnosis, levels = c("B", "M"))
str(wdbc.data)
levels(wdbc.data$Diagnosis)
summary(wdbc.data)
par(mfrow=c(1,1))
ggplot(wdbc.data, aes(x = Diagnosis, fill = Diagnosis)) + geom_bar() + ggtitle("Count of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
par(mfrow=c(2,2))
ggplot(wdbc.data, aes(x = Diagnosis, y = Mean_Area, fill = Diagnosis)) + geom_boxplot() + ggtitle("Mean_Area of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
ggplot(wdbc.data, aes(x = Mean_Perimeter, fill = Diagnosis)) + geom_histogram(alpha = 0.5, position = "identity", bins = 30) + ggtitle("Mean_Perimeter of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
ggplot(wdbc.data, aes(x = Worst_Area, y = Mean_Area, color = Diagnosis)) + geom_point(alpha = 0.7) + ggtitle("Mean_Area vs Worst_Area of Benign and Malignant Tumors") + theme(plot.title = element_text(size = 8, hjust = 0.5))
hist(wdbc.data$Mean_Perimeter,
xlab = "Mean Perimeter",
main = "Histogram of Mean Perimeter",
col = "yellow")
hist(wdbc.data$Worst_Perimeter,
xlab = "Worst Perimeter",
main = "Histogram of Worst Perimeter",
col = "red")
hist(wdbc.data$Mean_Radius,,
xlab = "Mean Radius",
main = "Histogram of Mean Radius",
col = "green")
hist(wdbc.data$Worst_Area,,
xlab = "Mean Area",
main = "Histogram of Worst Area",
col = "orange")
# Count the number of benign and malignant tumors
diagnosis_counts <- table(wdbc.data$Diagnosis)
diagnosis_counts
t_test_area <- t.test(Mean_Area ~ Diagnosis, data = wdbc.data)
t_test_area
t_test_Worstarea <- t.test(Worst_Area ~ Diagnosis, data = wdbc.data)
t_test_Worstarea
corMatrix <- cor(wdbc.data[, sapply(wdbc.data, is.numeric)])
ggplot(melt(corMatrix), aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0)+ theme_minimal()+
theme(
axis.text.x = element_text(angle = 90, hjust = 1, size = 5),
axis.text.y = element_text(size = 5),
axis.title = element_blank()
)
highCorr <- which(abs(corMatrix) > 0.8 & abs(corMatrix) < 1, arr.ind = TRUE)
correlated_pairs <- data.frame(
Feature1 = rownames(corMatrix)[highCorr[, 1]],
Feature2 = colnames(corMatrix)[highCorr[, 2]],
Correlation = corMatrix[highCorr]
)
knitr::kable(head(correlated_pairs), caption = "Highly Correlated Features")
fullModel <- lm(Mean_Radius ~ ., data = wdbc.data)
backwardModel <- step(fullModel, direction = "backward")
summary(backwardModel)
finalModel<- glm(Diagnosis ~ Mean_Area + Mean_Smoothness + Mean_FractalDimension +
SE_Radius + SE_Texture + SE_Concavity + SE_ConcavePoints +
SE_FractalDimension + Worst_Texture + Worst_Smoothness + Worst_Compactness,
family = binomial(link = "logit"), data = wdbc.data)
summary(finalModel)
predicted_probs <- predict(finalModel, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, "M", "B")
table(Predicted = predicted_classes, Actual = wdbc.data$Diagnosis)
# Load the dataset
data <- read.csv("wdbc.data", header = FALSE)
colnames(data) <- c("ID", "Diagnosis", paste0("Feature", 1:(ncol(data) - 2)))
# Convert diagnosis to a factor
data$Diagnosis <- as.factor(data$Diagnosis)
# Split the data into training and testing sets (70/30 split)
set.seed(123)
train_index <- createDataPartition(data$Diagnosis, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree
rpart.plot(decision_tree)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
confusionMatrix(tree_predictions, test_data$Diagnosis)
# Fit a Random Forest model
set.seed(123)
random_forest <- randomForest(Diagnosis ~ ., data = train_data, ntree = 100, mtry = sqrt(ncol(train_data) - 1))
# Evaluate model performance
rf_predictions <- predict(random_forest, test_data)
#confusionMatrix(rf_predictions, test_data$Diagnosis)
# Variable importance
#importance(random_forest)
varImpPlot(random_forest, main = "Feature Importance")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 23, 24)]
#print(feature_names) SE_Symmetry, Worst_Radius, Worst_Texture
# ROC Curve (only for binary classification)
if (length(levels(test_data$Diagnosis)) == 2) {
rf_probabilities <- predict(random_forest, test_data, type = "prob")[, 2] # Probabilities for the positive class
roc_curve <- roc(test_data$Diagnosis, rf_probabilities, levels = rev(levels(test_data$Diagnosis)), direction = "<")
plot(roc_curve, col = "#1c61b6", lwd = 2, main = "ROC Curve")
auc_value <- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), col = "#1c61b6", lwd = 2)
}
# OOB Error Rate Plot
plot(random_forest, main = "OOB Error Rate")
legend("topright", legend = colnames(random_forest$err.rate), fill = 1:ncol(random_forest$err.rate), cex = 0.8)
# Prepare the data: Remove ID column and ensure all predictors are numeric
knn_train_data <- train_data[, -1]  # Remove ID column
knn_test_data <- test_data[, -1]   # Remove ID column
# Scale the features
knn_train_scaled <- scale(knn_train_data[, -1])  # Exclude Diagnosis column
knn_test_scaled <- scale(knn_test_data[, -1])   # Exclude Diagnosis column
# Set up cross-validation for k values
set.seed(123)
k_values <- seq(1, 20, by = 2)  # Try odd values of k from 1 to 20
cv_results <- data.frame(k = k_values, Accuracy = numeric(length(k_values)))
# Perform cross-validation
for (i in seq_along(k_values)) {
k <- k_values[i]
# Apply KNN with cross-validation
cv_predictions <- knn.cv(train = knn_train_scaled,
cl = knn_train_data$Diagnosis,
k = k)
# Calculate accuracy
cv_accuracy <- sum(cv_predictions == knn_train_data$Diagnosis) / length(cv_predictions)
# Store results
cv_results$Accuracy[i] <- cv_accuracy
}
# Find the best k value
best_k <- cv_results$k[which.max(cv_results$Accuracy)]
cat("Optimal k:", best_k, "\\n")
# Refit the KNN model using the optimal k
knn_optimized_predictions <- knn(train = knn_train_scaled,
test = knn_test_scaled,
cl = knn_train_data$Diagnosis,
k = best_k)
# Evaluate performance of the optimized KNN
confusionMatrix(knn_optimized_predictions, knn_test_data$Diagnosis)
# Plot the accuracy against k values
ggplot(cv_results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
ggtitle("Cross-Validation Accuracy vs k") +
xlab("k") +
ylab("Accuracy")
# Prepare the data: Remove ID column and ensure all predictors are numeric
knn_train_data <- train_data[, -1]  # Remove ID column
knn_test_data <- test_data[, -1]   # Remove ID column
# Scale the features for KNN
knn_train_scaled <- scale(knn_train_data[, -1])  # Exclude Diagnosis column
knn_test_scaled <- scale(knn_test_data[, -1])   # Exclude Diagnosis column
# Define the K value (start with k = 5)
k <- 5
set.seed(123)
# Apply KNN
knn_predictions <- knn(train = knn_train_scaled,
test = knn_test_scaled,
cl = knn_train_data$Diagnosis,
k = k)
# Compute confusion matrix
conf_matrix <- confusionMatrix(knn_predictions, knn_test_data$Diagnosis)
# Extract confusion matrix as a table
conf_table <- as.data.frame(conf_matrix$table)
# Rename columns for clarity
colnames(conf_table) <- c("Prediction", "Reference", "Freq")
# Create a heatmap of the confusion matrix
ggplot(conf_table, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
ggtitle("Confusion Matrix Heatmap for KNN") +
xlab("Actual Class") +
ylab("Predicted Class") +
theme_minimal()
evaluate_svm_kernel <- function(kernel, train_data, test_data, target_column) {
set.seed(123)
model <- svm(
as.formula(paste(target_column, "~ .")),
data = train_data,
kernel = kernel,
probability = TRUE
)
predictions <- predict(model, test_data)
accuracy <- mean(predictions == test_data[[target_column]])
list(kernel = kernel, accuracy = accuracy)
}
# Step 1: Identify the best kernel
kernels <- c("linear", "radial", "polynomial", "sigmoid")
results <- lapply(kernels, function(kernel) {
evaluate_svm_kernel(kernel, train_data, test_data, "Diagnosis")
})
accuracy_results <- data.frame(
Kernel = sapply(results, function(res) res$kernel),
Accuracy = sapply(results, function(res) res$accuracy)
)
best_kernel <- accuracy_results[which.max(accuracy_results$Accuracy), "Kernel"]
cat("Best Kernel:", best_kernel, "\n")
# Step 2: Hyperparameter tune the best kernel
set.seed(123)
tuned_model <- tune(
svm,
Diagnosis ~ .,
data = train_data,
kernel = best_kernel,
ranges = list(
cost = 2^(-1:2),
gamma = 2^(-2:1)
)
)
best_model <- tuned_model$best.model
best_parameters <- tuned_model$best.parameters
cat("Best Parameters for Kernel", best_kernel, ":\n")
print(best_parameters)
# Step 3: Evaluate the tuned model
tuned_predictions <- predict(best_model, test_data)
confusion_matrix <- table(Predicted = tuned_predictions, Actual = test_data$Diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Tuned Model Accuracy:", accuracy, "\n")
# Convert confusion matrix to a dataframe for visualization
confusion_df <- as.data.frame(as.table(confusion_matrix))
colnames(confusion_df) <- c("Predicted", "Actual", "Freq")
# Create the heatmap
ggplot(confusion_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
ggtitle("Confusion Matrix Heatmap for Tuned SVM Model") +
xlab("Actual Class") +
ylab("Predicted Class") +
theme_minimal()
# Decision Tree Accuracy
dt_accuracy <- confusionMatrix(tree_predictions, test_data$Diagnosis)$overall["Accuracy"]
# Random Forest Accuracy
rf_accuracy <- confusionMatrix(rf_predictions, test_data$Diagnosis)$overall["Accuracy"]
# KNN Accuracy
knn_accuracy <- confusionMatrix(knn_predictions, knn_test_data$Diagnosis)$overall["Accuracy"]
# Compare the accuracies
cat("Decision Tree Accuracy:", dt_accuracy)
cat("Random Forest Accuracy:", rf_accuracy)
cat("KNN Accuracy:", knn_accuracy)
cat("Tuned SVM Linear Kernel Accuracy: 0.9705882")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(e1071)
library(car)
library(caret)
library(randomForest)
library(pROC)
library(class)
library(reshape2)
setwd("\\Users\\mnusa\\STAT 385\\Final Project")
wdbc.data <- read.table(file="wdbc.data",sep = ",",header=FALSE)
str(wdbc.data)
ncol(wdbc.data)
sum(is.na(wdbc.data))
#renaming the columns, to make sense
column_names <- c(
"ID", "Diagnosis",
paste(rep(c("Mean", "SE", "Worst"), each = 10),
c("Radius", "Texture", "Perimeter", "Area",
"Smoothness", "Compactness", "Concavity",
"ConcavePoints", "Symmetry", "FractalDimension"), sep = "_")
)
colnames(wdbc.data) <- column_names
wdbc.data$Diagnosis <- factor(wdbc.data$Diagnosis, levels = c("B", "M"))
str(wdbc.data)
levels(wdbc.data$Diagnosis)
summary(wdbc.data)
par(mfrow=c(1,1))
ggplot(wdbc.data, aes(x = Diagnosis, fill = Diagnosis)) + geom_bar() + ggtitle("Count of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
par(mfrow=c(2,2))
ggplot(wdbc.data, aes(x = Diagnosis, y = Mean_Area, fill = Diagnosis)) + geom_boxplot() + ggtitle("Mean_Area of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
ggplot(wdbc.data, aes(x = Mean_Perimeter, fill = Diagnosis)) + geom_histogram(alpha = 0.5, position = "identity", bins = 30) + ggtitle("Mean_Perimeter of Benign vs Malignant Tumors") + theme(plot.title = element_text(size = 10, hjust = 0.5))
ggplot(wdbc.data, aes(x = Worst_Area, y = Mean_Area, color = Diagnosis)) + geom_point(alpha = 0.7) + ggtitle("Mean_Area vs Worst_Area of Benign and Malignant Tumors") + theme(plot.title = element_text(size = 8, hjust = 0.5))
hist(wdbc.data$Mean_Perimeter,
xlab = "Mean Perimeter",
main = "Histogram of Mean Perimeter",
col = "yellow")
hist(wdbc.data$Worst_Perimeter,
xlab = "Worst Perimeter",
main = "Histogram of Worst Perimeter",
col = "red")
hist(wdbc.data$Mean_Radius,,
xlab = "Mean Radius",
main = "Histogram of Mean Radius",
col = "green")
hist(wdbc.data$Worst_Area,,
xlab = "Mean Area",
main = "Histogram of Worst Area",
col = "orange")
# Count the number of benign and malignant tumors
diagnosis_counts <- table(wdbc.data$Diagnosis)
diagnosis_counts
t_test_area <- t.test(Mean_Area ~ Diagnosis, data = wdbc.data)
t_test_area
t_test_Worstarea <- t.test(Worst_Area ~ Diagnosis, data = wdbc.data)
t_test_Worstarea
corMatrix <- cor(wdbc.data[, sapply(wdbc.data, is.numeric)])
ggplot(melt(corMatrix), aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0)+ theme_minimal()+
theme(
axis.text.x = element_text(angle = 90, hjust = 1, size = 5),
axis.text.y = element_text(size = 5),
axis.title = element_blank()
)
highCorr <- which(abs(corMatrix) > 0.8 & abs(corMatrix) < 1, arr.ind = TRUE)
correlated_pairs <- data.frame(
Feature1 = rownames(corMatrix)[highCorr[, 1]],
Feature2 = colnames(corMatrix)[highCorr[, 2]],
Correlation = corMatrix[highCorr]
)
knitr::kable(head(correlated_pairs), caption = "Highly Correlated Features")
fullModel <- lm(Mean_Radius ~ ., data = wdbc.data)
backwardModel <- step(fullModel, direction = "backward")
summary(backwardModel)
finalModel<- glm(Diagnosis ~ Mean_Area + Mean_Smoothness + Mean_FractalDimension +
SE_Radius + SE_Texture + SE_Concavity + SE_ConcavePoints +
SE_FractalDimension + Worst_Texture + Worst_Smoothness + Worst_Compactness,
family = binomial(link = "logit"), data = wdbc.data)
summary(finalModel)
predicted_probs <- predict(finalModel, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, "M", "B")
table(Predicted = predicted_classes, Actual = wdbc.data$Diagnosis)
# Load the dataset
data <- read.csv("wdbc.data", header = FALSE)
colnames(data) <- c("ID", "Diagnosis", paste0("Feature", 1:(ncol(data) - 2)))
# Convert diagnosis to a factor
data$Diagnosis <- as.factor(data$Diagnosis)
# Split the data into training and testing sets (70/30 split)
set.seed(123)
train_index <- createDataPartition(data$Diagnosis, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree
rpart.plot(decision_tree)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 28)]
print(feature_names) #SE_Symmetry, Worst_Radius, Worst_Texture
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree
rpart.plot(decision_tree)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 28)]
#print(feature_names) SE_Symmetry, Worst_Compactness
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree with a Legend
rpart.plot(
decision_tree,
main = "Decision Tree with Feature Legend",
extra = 104, # Adds class probabilities and the class label
under = TRUE # Adds the response label under the node
)
# Add a legend manually
legend(
"topright", # Position of the legend
legend = c("Feature 21: SE_Symmetry", "Feature 24: Worst_Compactness"),
col = c("black"), # Text color
cex = 0.8, # Adjust text size
box.lty = 0 # Removes the legend box
)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 28)]
#print(feature_names) SE_Symmetry, Worst_Compactness
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree
rpart.plot(decision_tree)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 28)]
#print(feature_names) SE_Symmetry, Worst_Compactness
# Fit a Decision Tree model
set.seed(123)
decision_tree <- rpart(Diagnosis ~ ., data = train_data, method = "class")
# Visualize the Decision Tree with a Legend
rpart.plot(
decision_tree,
main = "Decision Tree with Feature Legend",
)
# Add a legend manually
legend(
"topright", # Position of the legend
legend = c("Feature 21: SE_Symmetry", "Feature 24: Worst_Compactness"),
col = c("black"), # Text color
cex = 0.8, # Adjust text size
box.lty = 0 # Removes the legend box
)
# Make predictions and evaluate performance
tree_predictions <- predict(decision_tree, test_data, type = "class")
# Print the names of features 21, 23, and 24
feature_names <- colnames(wdbc.data)[c(21, 28)]
#print(feature_names) SE_Symmetry, Worst_Compactness
confusionMatrix(tree_predictions, test_data$Diagnosis)
